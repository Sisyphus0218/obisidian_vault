2020K8009929017 侯昱帆

1.请介绍 MPI 中阻塞发送 MPI_Send / 阻塞接收 MPI_Recv 与非阻塞发送 MPI_Isend / 非阻塞接收MPI_Irecv 的区别。
答：
阻塞方式：必须等到消息从本地送出之后才可以执行后续的语句，保证了缓冲区等资源可再用。
非阻塞方式：无须等到消息从本地送出就可执行后续的语句，从而允许通信和计算的重叠，但非阻塞调用的返回并不保证资源的可再用性。

[[第10章 并行编程基础#点到点通信]]

2.请介绍什么是归约 (Reduce) 操作，MPI 和 OpenMP 中分别采用何种函数或者子句来实现归约操作。
答：
归约：对序列中的元素重复应用合并操作，从而从元素序列中产生一个单一结果。
MPI：MPI_REDUCE、MPI_ALLREDUCE
OpenMP：reduction子句

[[第10章 并行编程基础#集体通信]] [[第10章 并行编程基础#数据域属性子句]]

>归约操作是指在分布在不同进程中的数据间进行交互的运算，常用的运算有求和、求最大或最小值等。
>OpenMP中使用reduction子句归约其列表中出现的变量。每个线程复制一个私有的变量列表。在归约的最后，归约的变量可以应用于所有的共享变量的私有变量列表中，最终结果写到全局共享变量。
>MPI中使用MPI_REDUCE、MPI_ALLREDUCE函数进行规约，包括求和、求积等。


3.请介绍什么是栅障 (Barrier) 操作，MPI 和 OpenMP 中分别采用何种函数或者命令来实现栅障。
答：
栅障：允许每个线程等待，直到所有的合作线程都到达某一点，然后从该点继续执行。
MPI：MPI_Barrier
OpenMP：barrier编译指导语句

[[第10章 并行编程基础#集体通信]] [[第10章 并行编程基础#同步结构]]

>当有一个barrier操作时，线程必须要等到所有的其他线程也到达这个barrier操作时才能继续执行，然后所有线程并行执行barrier操作之后的代码。

4.下面的 MPI 程序片段是否正确？请说明理由。假定只有 2 个进程正在运行且 mypid 为每个进程的进程号。
```
If(mypid==0) {
	MPI_Bcast(buf0,count,type,0,comm,ierr);
	MPI_Send(buf1,count,type,1,tag,comm,ierr);
} else {
	MPI_Recv(buf1,count,type,0,tag,comm,ierr);
	MPI_Bcast(buf0,count,type,0,comm,ierr);
}
```

答：
MPI_Bcase属于集体通信，所有的进程都必须执行集体通信函数。
MPI_Send属于点对点通信，且是阻塞机制，须等到消息从本地送出后才可执行后续的语句。
故0号进程执行MPI_Bcast后，等待1号进程执行MPI_Bcast。
1号进程执行MPI_Recv后，等待0号进程执行MPI_Send。
这样形成了死锁。

[[第10章 并行编程基础#集体通信]] [[第10章 并行编程基础#点到点通信]]

5.矩阵乘是数值计算中的重要运算。假设有一个 m×p 的矩阵 A，还有一个 p×n 的矩阵 B。令 C 为矩阵 A 与 B 的乘积，即 C=AB。表示矩阵在 (i, j) 位置处的值，则 0≤i≤m-1，0≤j≤n-1。请采用OpenMP，将矩阵 C 的计算并行化。假设矩阵在存储器中按行存放。
答：
```
#include <stdio.h>
#include <omp.h>

#define m 2000
#define p 1500
#define n 1000

double A[m][p],B[p][n],C[m][n];

int main()
{
	int i,j,k;
	
	//初始化矩阵 A 和矩阵 B
	Init_A();
	Init_B();
	
	//并行计算矩阵 C
	#pragma omp parallel for shared(A,B,C) private(i,j,k)
	for(i=0;i<m;i++)
		for(j=0;j<n;j++)
		{
			C[i][j] = 0;
			for(k=0;k<p;k++)
				C[i][j]+=A[i][k]* B[k][j];
		}
	
	return 0;
}
```

[[第10章 并行编程基础#OpenMP 例]]

6.请采用 MPI 将上题中矩阵 C 的计算并行化，并比较 OpenMP 与 MPI 并行程序的特点。
答：
```
#include <stdio.h>
#include “mpi.h”

#define m 2000
#define p 1500
#define n 1000

int main(int argc, char **argv)
{
	double *A,*B,*C;
	int i,j,k;
	int ID,num_procs,line;
	MPI_Status status;
	
	MPI_Init(&argc,&argv);                     // 初始化 MPI 环境
	MPI_Comm_rank(MPI_COMM_WORLD,&ID);         // 获取当前进程号
	MPI_Comm_size(MPI_COMM_WORLD,&num_procs);  // 获取进程数目
	
	//分配数据空间
	A = (double *)malloc(sizeof(double)*m*p);
	B = (double *)malloc(sizeof(double)*p*n);
	C = (double *)malloc(sizeof(double)*m*n);
	line = m/num_procs;  // 按进程数来划分数据
	
	if(ID==0)  // 节点0,主进程
	{ 
		// 初始化数组
		Init_A();
		Init_B();
		
		// 将矩阵 A、B 的相应数据发送给从进程
		for(i=1;i<num_procs;i++) 
		{
			MPI_Send(B,p*n,MPI_DOUBLE,i,0,MPI_COMM_WORLD);
			MPI_Send(A+(i-1)*line*p,line*p,MPI_DOUBLE,i,1,MPI_COMM_WORLD);
		}
		
		// 接收从进程的计算结果
		for(i=1;i<num_procs;i++)
			MPI_Recv(C+(i-1)*line*n,line*n,MPI_DOUBLE,i,2,MPI_COMM_WORLD,&status);
		
		// 计算剩下的数据
		for(i=(num_procs-1)*line;i<m;i++)
			for(j=0;j<n;j++) 
			{
				C[i*n+j]=0;
				for(k=0;k<p;k++)
					C[i*n+j]+=A[i*p+k]*B[k*n+j];
			}
	}
	else 
	{
		// 其他进程接收数据,计算结果,发送给主进程
		MPI_Recv(B,p*n,MPI_DOUBLE,0,0,MPI_COMM_WORLD,&status);
		MPI_Recv(A+(ID-1)*line*p,line*p,MPI_DOUBLE,0,1,MPI_COMM_WORLD,&status);
		
		for(i=(ID-1)*line;i<ID*line;i++)
			for(j=0;j<n;j++) 
			{
				C[i*n+j]=0;
				for(k=0;k<p;k++)
					C[i*n+j]+=A[i*p+k]*B[k*n+j];
			}
		
		MPI_Send(C+(num_procs-1)*line*n,line*n,MPI_DOUBLE,0,2,MPI_COMM_WORLD);
	}
	
	MPI_Finalize();
	Return 0;
}
```

OpenMP采用共享存储的方式，利用制导语句，程序会自动并行化运行，代码较为简单。
MPI采用消息传递的方式，需要程序员调用MPI函数使程序并行，代码较为复杂。

[[第10章 并行编程基础#MPI 例]]

>  OpenMP和MPI特点如下

| |并行粒度|存储方式|数据分配方式|扩展性|编程复杂性|
|---|---|---|---|---|---|
|OpenMP|线程|共享存储|隐式|差|简单|
|MPI|进程|分布式|显式|好|复杂|

7.分析一款 GPU 的存储层次。
答：Fermi GPU 体系结构的存储层次由每个 SM 的寄存器堆、每个 SM 的一级 Cache、统一的二级 Cache 和全局存储组成。
1. 寄存器。每个 SM 有 32K 个 32 位寄存器，每个线程可以访问自己的私有寄存器，随线程数目的不同，每个线程可访问的私有寄存器数目在 21~ 63 间变化。
2. 一级 Cache 和共享存储。每个 SM 有片上高速存储，主要用来缓存单线程的数据或者用于多线程间的共享数据，可以在一级 Cache 和共享存储之间进行配置。
3. 二级 Cache。768KB 统一的二级 Cache 在 16 个 SM 间共享，服务于所有到全局内存中的 load / store 操作。
4. 全局存储。所有线程共享的片外存储。


