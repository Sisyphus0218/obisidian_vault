#计算机体系结构-并行处理结构
## 10.1 程序的并行行为
### 10.1.1 指令级并行性
指令级并行性 (Instruction Level Parallelism，简称 ILP)：指令之间的并行执行。当指令之间不存在相关时，这些指令可以在处理器流水线上重叠起来并行执行。 

在程序运行中，如果必须等前一条指令执行完成后，才能执行后一条指令，那么这两条指令是相关的。 
指令相关主要包括数据相关、控制相关和结构相关。 
1. 数据相关包括写后读 (RAW) 相关、读后写 (WAR) 相关和写后写 (WAW) 相关。RAW 相关是真正的数据相关，因为存在真正的数据传递关系；WAR 相关和 WAW 相关又称为假相关或者名字相关，指令之间实际不存在数据传递。 
2. 控制相关主要是由于存在分支指令，一条指令的执行取决于该分支指令的执行结果，则这两条指令之间存在控制相关。 
3. 结构相关是指两条指令同时需要流水线中的同一个功能部件。
在这些相关中，RAW 数据相关和控制相关是真正制约指令级并行执行的相关。指令相关容易引起流水线阻塞，从而降低流水线效率。

现代处理器采用多种微结构设计技术挖掘指令级并行性：
1. 指令流水线重叠执行多条不相关的指令；
2. 多发射技术允许一个时钟周期执行多条指令，类似于 “多车道”；
3. 动态调度允许后续指令越过前面被阻塞的指令继续被调度执行，相当于允许 “超车”；
4. 寄存器重命名可以消除 RAW 和 WAW 的假相关并支持猜测执行；
5. 转移猜测技术可以猜测分支指令的方向和目标，在分支指令还未执行完之前获取更多可执行指令。以减少控制相关造成的指令流水线阻塞。 

### 10.1.2 数据级并行性
数据级并行性 (Data Level Parallelism，简称 DLP) ：对集合或者数组中的元素同时执行相同的操作。 
这种并行性通常来源于程序中的循环语句。 

下述代码就是一个数据并行的例子。 对于数组 local 中的元素 local\[i\], 执行相同的操作 “( i+0. 5)∗w”。 
可以采用将不同的数据分布到不同的处理单元的方式来实现数据级并行
```
for(i=0; i<N; i++)
{
	local[i] = (i+0.5)*w;
}
```

数据级并行性比较易于处理，可以在计算机体系结构的多个层次来利用数据级并行性。
1. 可以在处理器中设计向量功能部件，采用 SIMD 设计方法，如一个 256 位向量部件一次可以执行 4 个 64 位的操作；
2. 设计专门的向量处理器，如 CRAY 公司的 CRAY- 1、 CRAY- 2、X-MP、 Y-MP 等；
3. 在多处理器中，可以采用 SPMD ( Single Program Multi- Data) 的编程方式，将数据分布到不同的处理器上执行同一个程序控制流。

### 10.1.3 任务级并行性
任务级并行性 (Task Level Parallelism) ：将不同的任务 (进程或者线程) 分布到不同的处理单元上执行。 
针对任务表现为进程或者线程，任务级并行性可分为进程级并行性或者线程级并行性。 

eg. 对于一个双处理器系统，当处理器 ID为 a 时执行任务 A；当处理器 ID 为 b 时执行任务 B。
```
if(processor_ID=="a") {
	task A;
}else if (processor_ID=="b"){
	task B;
}
```

在并行处理系统中，挖掘任务并行性就是让每个处理器执行不同的线程或进程。这些线程或者进程可以执行相同或者不同的代码。通常情况下，不同线程或者进程之间还需要相互通信来协作完成整个程序的执行。 

## 10.2 并行编程模型 
### 10.2.1 单任务数据并行模型 
数据并行 (Data Parallel) 模型是指对集合或者数组中的元素同时 (即并行) 执行相同操作。

单任务数据并行编程模型具有以下特点:
1. 单线程 (Single Threading)。从程序员的角度，一个数据并行程序只由一个线程执行，具有单一控制线；就控制流而言，一个数据并行程序就像一个顺序程序一样。
2. 同构并行 (Identical Parallel)。数据并行程序的一条语句，同时作用在不同数组元素或者其他聚合数据结构，在数据并行程序的每条语句之后，均有一个隐式同步。
3. 全局命名空间 (Global Naming Space)。数据并行程序中的所有变量均在单一地址空间内，所有语句可访问任何变量而只要满足通常的变量作用域规则即可。
4. 隐式相互作用 (Implicit Interaction)。因为数据并行程序的每条语句结束时存在一个隐含的栅障 (Barrier)，所以不需要显式同步；通信可以由变量指派而隐含地完成。
5. 隐式数据分配 (Implicit Data Allocation)。程序员没必要明确指定如何分配数据，可将改进数据局部性和减少通信的数据分配方法提示给编译器。

### 10.2.2 多任务共享存储编程模型 
运行在各处理器上的进程 (或者线程) 可以通过读/ 写共享存储器中的共享变量来相互通信。

特点：
1. 多线程、异步执行的
2. 具有一个单一的全局名字空间（与数据并行模型类似）
3. 数据分配：不需要显式的分配数据
4. 负载分配：可以显式的分配也可以隐式的分配
5. 通信通过共享的读/写变量隐式的完成
6. 同步必须是显式的，以保持进程执行的正确顺序

共享存储编程模型如 Pthreads 和 OpenMP 等。

### 10.2.3 多任务消息传递编程模型 
在不同处理器上执行的进程，通过网络传递消息相互通信

特点：
1. 多进程 (Multiprocess)。消息传递并行程序由多个进程组成，每个进程都有自己的控制流且可执行不同代码；多程序多数据 (Multiple Program Multiple Data，简称 MPMD) 并行和单程序多数据 (SPMD) 并行均可支持。
2. 异步并行性 (Asynchronous Parallelism)。消息传递并行程序的各进程彼此异步执行，使用诸如栅障和阻塞通信等方式来同步各个进程。
3. 独立的地址空间 (Separate Address Space)。消息传递并行程序的进程具有各自独立的地址空间，一个进程的数据变量对其他进程是不可见的，进程的相互作用通过执行特殊的消息传递操作来实现。
4. 显式相互作用 (Explicit Interaction)。程序员必须解决包括数据映射、通信、同步和聚合等相互作用问题；计算任务分配通过拥有者-计算 (Owner-Compute) 规则来完成，即进程只能在其拥有的数据上进行计算。
5. 显式分配 (Explicit Allocation)。计算任务和数据均由用户显式地分配给进程，为了减少设计和编程的复杂性，用户通常采用单一代码方法来编写 SPMD 程序。

典型的消息传递编程模型包括 MPI 和 PVM。

### 10.2.4 共享存储与消息传递编程模型的编程复杂度 
采用共享存储与消息传递编程模型编写的并行程序是在多处理器并行处理系统上运行的。

##### 结构
从结构的角度看，多处理器系统可分为共享存储系统和消息传递系统两类。 
1. 共享存储：所有处理器共享主存储器，每个处理器都可以把信息存入主存储器，或从中取出信息，处理器之间的通信通过访问共享存储器来实现。 
2. 消息传递：每个处理器都有一个只有自己才能访问的局部存储器，处理器之间的通信必须通过显式的消息传递来进行。 

消息传递和共享存储系统的结构如图所示。 
消息传递系统中，每个处理器的存储器是单独编址的；在共享存储系统中，所有存储器统一编址。 
![[Pasted image 20221211141004.png]]

##### 编程模型
1. 消息传递：程序员需要对计算任务和数据进行划分，并安排并行程序执行过程中进程间的所有通信。 
2. 共享存储：由于程序的多进程 (或者线程) 之间存在一个统一编址的共享存储空间，程序员只需进行计算任务划分，不必进行数据划分，也不用确切地知道并行程序执行过程中进程间的通信。
消息传递系统的可伸缩性通常比共享存储系统要好, 可支持更多处理器。

##### 通信
从进程 (或者线程) 间通信的角度看，消息传递并行程序比共享存储并行程序复杂一些，体现为两方面：
1. 空间管理：发送数据的进程需要关心自己产生的数据被谁用到，接收数据的进程需要关心它用到了谁产生的数据。
2. 时间管理：发送数据的进程通常需要在数据被接收后才能继续，接收数据的进程通常需要等到接收数据后才能继续。 

在共享存储并行程序中，各进程间的通信通过访问共享存储器完成，程序员只需考虑进程间同步，不用考虑进程间通信。 尤其是比较复杂的数据结构的通信，如 ``struct{int∗pa; int∗pb; int∗pc;}``，消息传递并行程序比共享存储并行程序复杂。

对于一些在编程时难以确切知道进程间通信的程序，用消息传递的方法很难进行并行化，
如 ``{for(i,j) {x =…; y =…; a[i][j]=b[x][y];}}``。这段代码中，通信内容在程序运行时才能确定，编写代码时难以确定，改写成消息传递程序就比较困难。

##### 数据划分
1. 消息传递：必须考虑诸如数组名称以及下标变换等因素，在将一个串行程序改写成并行程序的过程中，需要修改大量的程序代码。
2. 共享存储：进行串行程序的并行化改写时，不用进行数组名称以及下标变换，对代码的修改量少。 

虽说共享存储程序无须考虑数据划分，但是在实际应用中，为了获得更高的系统性能，有时也需要考虑数据分布，使得数据尽量分布在对其进行计算的处理器上，例如 OpenMP 中就有进行数据分布的扩展制导。不过相对于消息传递程序中的数据划分要简单得多。

##### 总结
共享存储编程像 BBS 应用，一个人向 BBS 上发帖子，其他人都看得见；
消息传递编程则像电子邮件 (E-mail)，你得想好给谁发邮件，发什么内容。

##### 例子
**积分求圆周率**
$$
\pi=4\int_0^1 \frac{1}{1+x^2}dx=\sum_{i=1}^N \frac{4}{1+(\frac{i-0.5}{N})^2}\times\frac{1}{N}
$$
在上式中，N 值越大，误差越小。如果 N 值很大，计算时间就很长。
可以通过并行处理，让每个进程计算其中的一部分，最后把每个进程计算的值加在一起来减少运算时间。 

并行程序采用 SPMD (Single Program Multiple Data) 的模式，即每个进程都运行同一个程序，但处理不同的数据。 
numprocs：参与运算的进程个数，所有参与运算的进程都有相同的 numprocs 值; 
myid：参与运算的进程的编号，每个进程都有自己的编号 (一般并行编程系统都会提供接口函数让进程知道自己的编号)。
例如，如果有 4 个进程参与运算，则每个进程的 numprocs 都是 4，而每个进程的 myid 号分别为0、1、2、3。 

共享存储并行程序：
由 jia_alloc() 分配空间的变量 pi 是所有进程共享的，只有一份；其他变量是每个进程局部的，每个进程都有一份。
每个进程根据 numprocs 和 myid 号分别计算部分圆周率值，最后通过一个临界区的机制把所有进程的计算结果加在一起。 
jia_lock()和 jia_unlock()是一种临界区的锁机制，保证每次只有一个进程进入这个临界区，这样才能把所有进程的结果依次加在一起，不会互相冲掉。 

消息传递并行程序：
由 malloc() 分配空间的变量每个进程都有独立的一份，互相看不见。
每个进程算完部分结果后，通过归约操作 reduce()把所有进程的 mypi 加到 0 号进程的 pi 中。

![[Pasted image 20221211142510.png]]

**矩阵乘法**
共享存储并行程序：
由 jia_alloc() 分配的变量所有进程共享一份，而由malloc()分配的变量每个进程单独一份。
先由 0 号进程对 A、B、C 三个矩阵进行初始化，而其他进程通过 jia_barrier()语句等待。 
然后每个进程分别完成部分运算，再通过 jia_barrier() 等齐后由 0 号进程统一打印结果。 

消息传递并行程序：
与共享存储并行程序的最大区别是需要通过显式的发送语句 send 和接收语句 recv 进行多个进程之间的通信。 
先由 0 号进程进行初始化后发送给其他进程，每个进程分别算完后再发送给 0 号进程进行打印。 
要详细列出每次发送的数据大小和起始地址等信息，0 号进程接收的时候还要把从其他进程收到的数据拼接在一个矩阵中，比共享存储并行程序麻烦不少。

![[Pasted image 20221211142515.png]]

## 10.3 典型并行编程环境 
### 10.3.1 数据并行 SIMD 编程 
单指令流多数据流 (Single Instruction Multiple Data，简称 SIMD) 并行就是典型的数据并行技术。 
一条 SIMD 指令可以同时对一组数据进行相同的计算。

比如将两个数组 SRC0\[8\] 和 SRC1\[8\] 中的每个对应元素求和，将结果放入数组 RESULT 中。
对于传统的标量处理器平台，C 语言实现如下:

```
for(i=0; i<8; i++)
	RESULT[i] = SRC0[i] + SRC1[i];
```

在龙芯处理器平台上，用机器指令 (汇编代码) 实现该运算的代码如下 (这里假设＄src0、＄src1、＄result 分别为存储了 SRC0、SRC1 和 RESULT 数组起始地址的通用寄存器)：

```
	li     $r4, 0x0
	li     $r5, 0x8
1:  addi.d $src0, $src0, $r4
	addi.d $src1, $src1, $4
	addi.d $result, $result, $4
	ld.b   $r6, $src0, 0x0
	ld.b   $r7, $src1, 0x0
	add.d  $r6, $r6, $r7
	st.b   $r6, $result, 0x0
	addi.d $r4, $r4, 0x1
	blt    $r4, $r5, 1b
```

如果采用龙芯处理器的 SIMD 指令编写程序，
1. 将 SRC0\[8\]和 SRC1\[8\]一次性加载到龙芯处理器的向量寄存器 (龙芯向量寄存器复用了浮点寄存器) 中；
2. 只需要一条 paddb 指令完成上述 8 个对应数组项的求和；
3. 只需要一条 store 指令将结果存回 RESULT\[8\]数组所在的内存空间中。 
该实现的机器指令序列如下:

```
vld    $vr0, $src0, 0
vld    $vr1, $src1, 0
vadd.b $vr0, $vr0, $vr1
vst    $vr0, $result, 0
```

图 10.6 简要示意了采用传统 SISD 指令和 SIMD 指令实现上述 8 个对应数组项求和的执行控制流程。
![[Pasted image 20221211143949.png]]

### 10.3.2 POSIX 编程标准 
##### 线程管理
![[Pasted image 20221211150701.png]]

##### 线程调度
pthread_yield()：使调用者将处理器让位于其他线程；
pthread_cancel()：中止指定的线程。

##### 线程同步
![[Pasted image 20221211150757.png]]

##### pthread 例
eg. 积分求$\pi$
```
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>
#define NUM_THREADS 4 // 假设线程数目为 4

int num_steps = 1000000;
double step = 0.0, sum = 0.0;
pthread_mutex_t mutex;

void *countPI(void *id) 
{
	int index = (int) id;
	int start = index*(num_steps/NUM_THREADS);
	int end;
	
	double x = 0.0, y = 0.0;
	
	if (index == NUM_THREADS-1)
		end = num_steps;
	else
		end = start+(num_steps/NUM_THREADS);
		
	for (int i=start; i<end; i++)
	{
		x = (i+0.5)*step;
		y +=4.0/(1.0+x*x);
	}
	
	pthread_mutex_lock(&mutex);
	sum += y;
	pthread_mutex_unlock(&mutex);
}

int main() {
	int i;
	double pi;
	step = 1.0 / num_steps;
	sum = 0.0;
	pthread_t tids[NUM_THREADS];
	
	pthread_mutex_init(&mutex, NULL);
	for(i=0; i<NUM_THREADS; i++) 
	{
		pthread_create(&tids[i], NULL, countPI, (void *)i);
	}
	for(i=0; i<NUM_THREADS; i++)
		pthread_join(tids[i], NULL);
	pthread_mutex_destroy(&mutex);
	
	pi = step*sum;
	printf("pi %1f\n", pi);
	return 0;
}
```

矩阵乘法
```
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>

#define NUM_THREADS 4 // 假设线程数目为 4
#define n 1000
double *A,*B,*C;

void *matrixMult(void *id) // 计算矩阵乘
{
	int my_id = (int ) id;
	int i, j, k, start, end;
	
	// 计算进程负责的部分
	start = my_id*(n/NUM_THREADS);
	if(my_id == NUMTHREADS-1)
		end = n;
	else
		end = start+(n/NUM_THREADS);
		
	for(i=start; i<end; i++)
		for(j=0; j<n; j++) 
		{
			C[i*n+j] = 0;
			for(k=0; k<n; k++)
				C[i*n+j]+=A[i*n+k]*B[k*n+j];
		}
}

int main() 
{
	int i,j;
	pthread_t tids[NUM_THREADS];
	
	// 分配数据空间
	A = (double *)malloc(sizeof(double)*n*n);
	B = (double *)malloc(sizeof(double)*n*n);
	C = (double *)malloc(sizeof(double)*n*n);
	
	// 初始化数组
	for(i=0;i<n;i++)
		for(j=0;j<n;j++)
		{
			A[i*n+j] = 1.0;
			B[i*n+j] = 1.0;
		}
	
	for(i=0; i<NUM_THREADS; i++)
		pthread_create(&tids[i], NULL, matrixMult, (void *)i);
	for(i=0; i<NUM_THREADS; i++)
		pthread_join(tids[i], NULL);
		
	return 0;
}
```

### 10.3.3 OpenMP 标准
##### OpenMP 的并行执行模型
OpenMP 是一个基于线程的并行编程模型，一个 OpenMP 进程由多个线程组成，使用 fork-join 并行执行模型。 
OpenMP 程序开始于一个单独的主线程 (Master Thread)，主线程串行执行，遇到一个并行域 (Parallel Region) 开始并行执行。接下来的过程如下:
1. fork：主线程派生出一队并行的线程，并行域的代码在主线程和派生出的线程间并行执行。
2. join：当派生线程在并行域中执行完后或被阻塞或被中断，计算的结果被主线程收集，最后只有主线程在执行。

OpenMP 的并行化都是使用嵌入到 C/ C++或者 Fortran 语言的制导语句来实现的。OpenMP 程序的并行结构如下：
```
#include <omp.h>
main()
{
	int var1,var2,var3;
	…
	#pragma omp parallel private(var1,var2) shared(var3)
	{
		…
	}
	…
}
```

##### 编译制导语句
![[Pasted image 20221211151956.png]]

##### 并行域结构
一个并行域就是一个能被多个线程执行的程序块，是最基本的 OpenMP 并行结构。并行域的具体格式为：
```
#pragma omp parallel [if(scalar_expression) | num_threads(integer-expression) | default(shared|none) | private(list) | firstprivate(list) | shared(list) | copyin(list) | reduction(operator:list) | proc_bind(master|close|spread)] newline
```

当一个线程执行到 parallel 这个指令时，线程就会生成一列线程 (线程号从 0 到 n-1)，自己会成为主线程 (线程号为 0)。 
当并行域开始时，程序代码会被复制，每个线程都会执行该代码。
当并行域结束，会有一个栅障，只有主线程能够通过这个栅障。

##### 共享任务结构
共享任务结构将其内封闭的代码段划分给线程队列中的各线程执行。它不产生新的线程,
在进入共享任务结构时不存在栅障，但是在共享任务结构结束时存在一个隐含的栅障。

pragma omp parallel do/for：将循环分布到线程列中执行，表达数据并行
pragma omp parallel sections：将任务分成多个setcion，由不同线程执行，表达任务并行
pragma omp parallel single：由线程列中的一个线程串行执行
![[Pasted image 20221211152057.png]]

##### 同步结构
OpenMP 提供了多种同步结构来控制与其他线程相关的线程的执行。 
master编译制导语句：表明一个只能被主线程执行的域。
格式为：``#pragma omp master newline``
critical编译制导语句：表明域中代码一次只能由一个线程执行。
格式为： ``#pragma omp critical[name] newline``
barrier编译制导语句：同步线程队列中的所有线程。
格式为： ``#pragma omp barrier newline``
atomic编译制导语句：表明一个特别的存储单元只能原子的更新，而不允许让多个线程同时去写。
格式为：``#pragma omp atomic newline``

##### 数据域属性子句 
数据域属性子句用来定义变量范围，一般与parallel、for和sections语句配合使用
private 子句：表示它列出的变量对于每个线程是局部的。
shared 子句：表示它列出的变量被线程队列中的所有线程共享。
default 子句：表示并行域的所有变量的默认属性（如可以是private、shared、none）。
firstprivate 子句：含private子句的操作，初始化为并行域外同名变量值。
lastprivate 子句：含private子句的操作，并将值复制给并行域外同名变量。
threadprivate 编译制导语句：将全局变量变成每个线程私有。
copyin 子句：赋予线程中变量与主线程中threadprivate同名变量的值。
reduction 子句：用来归约其列表中出现的变量。

##### OpenMP 例
计算$\pi$
```
#include <stdio.h>
#include <omp.h>

int main()
{
	int i;
	int num_steps=1000000;
	double x,pi,step,sum=0.0;
	step = 1.0/(double)num_steps;
	
	#pragma omp parallel for private(i, x), reduction(+:sum)
	for(i=0; i<num_steps; i++)
	{ 
		x = (i+0.5)*step;
		sum = sum+4.0/(1.0+x*x);
	}
	
	pi = step*sum;
	
	printf(“pi %1f\n”,pi);
	return 0;
}
```

矩阵乘法
```
#include <stdio.h>
#include <omp.h>
#define n 1000

double A[n][n],B[n][n],C[n][n];

int main()
{
	int i,j,k;
	
	//初始化矩阵 A 和矩阵 B
	for(i=0;i<n;i++)
		for(j=0;j<n;j++) 
		{
			A[i][j] = 1.0;
			B[i][j] = 1.0;
		}
	
	//并行计算矩阵 C
	#pragma omp parallel for shared(A,B,C) private(i,j,k)
	for(i=0;i<n;i++)
		for(j=0;j<n;j++)
		{
			C[i][j] = 0;
			for(k=0;k<n;k++)
				C[i][j]+=A[i][k]* B[k][j];
		}
		
	return 0;
}
```

### 10.3.4 MPI 消息传递编程接口
##### 最基本的MPI
![[Pasted image 20221211154446.png]]

##### 点到点通信
1. 阻塞方式：等消息从本地送出之后才可以执行后续的语句，保证了缓冲区等资源可再用。
2. 非阻塞方式：不等消息从本地送出就可执行后续的语句，不保证资源的可再用性。

四种通信模式：
1. 标准模式：包括阻塞发送 MPI_Send、 阻塞接收 MPI_Recv、 非阻塞发送 MPI_Isend 和非阻塞接收 MPI_Irecv；
2. 缓冲模式：包括阻塞缓冲发送 MPI_Bsend 和非阻塞缓冲发送 MPI_Ibsend；
3. 同步模式：包括阻塞同步发送 MPI_Ssend 非阻塞同步发送 MPI_Issend；
4. 就绪模式：包括阻塞就绪发送 MPI_Rsend 和非阻塞就绪发送 MPI_Irsend，

解释：
1. 标准模式：是否缓存由MPI决定。
2. 缓冲模式：将送出的消息放在缓冲区内，发送者可以继续计算，多一次内存拷贝。
3. 同步模式：只有接收操作开始后，发送才能返回。
4. 就绪模式：只有接收操作启动后，发送操作才能开始。

##### 集体通信
1. 路障（MPI_BARRIER）：同步所有进程；
2. 广播（MPI_BCAST）：从一个进程发送一条数据给所有进程；
3. 收集（MPI_GATHER）：从所有进程收集数据到一个进程；
4. 散播（Scatter）：从一个进程散发多条数据给所有进程；
5. 归约（MPI_REDUCE、MPI_ALLREDUCE）：包括求和、求积等。

不同于点对点通信, 所有的进程都必须执行集体通信函数。 集体通信函数不需要同步操作就能使所有进程同步, 因此可能造成死锁。 这意味着集体通信函数必须在所有进程上以相同的顺序执行。

##### MPI 例
计算$\pi$
```
# include <stdio.h>
# include “mpi.h”

int main(int argc, char **argv)
{
	int num_steps=1000000;
	double x, pi, step, sum, sumallprocs;
	int i, start, end, temp;
	int num_procs; //组中的进程数量, 
	int ID;        //进程编号,范围为0到num_procs-1
	MPI_Status status;
	   
	// 初始化MPI环境
	MPI_Init(&argc,&argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &ID);          
	MPI_Comm_size(MPI_COMM_WORLD, num_procs);
	
	// 任务划分并计算
	step = 1.0/num_steps;
	start = ID*(num_steps/num_procs);
	if (ID == num_procs-1)  
	       end = num_steps; 
	else  
	       end = start + num_steps/num_procs;
	
	for(i=start; i<end;i++) 
	{
		x=(i+0.5)*step;
		sum += 4.0/(1.0+x*x);
	}
	MPI_Barrier(MPI_COMM_WORLD);
	MPI_Reduce(&sum, &sumallprocs, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
	
	if(ID==0) 
	{  
		pi = sumallprocs*step;
		printf(“pi %1f\n”, pi);   
	}
	      
	 MPI_Finalize();
	 return 0;
 }

```

矩阵乘法
```
#include <stdio.h>
#include “mpi.h”
#define n 1000
int main(int argc, char **argv) {
	double *A,*B,*C;
	int i, j, k, ID, num_procs, line;
	MPI_Status status；
	
	MPI_Init(&argc,&argv);                    // 初始化MPI环境
	MPI_Comm_rank(MPI_COMM_WORLD,&ID);        // 获取当前进程号
	MPI_Comm_size(MPI_COMM_WORLD,&num_procs); // 获取进程数目
	
	// 分配数据空间
	A = (double *)malloc(sizeof(double)*n*n);
	B = (double *)malloc(sizeof(double)*n*n);
	C = (double *)malloc(sizeof(double)*n*n);
	line = n/num_procs;  //按进程数来划分数据
	
	if(ID==0) //节点0，主进程
	{ 
		// 初始化数组
		for(i=0;i<n;i++) 
			for(j=0;j<n;j++)
			{ 
				A[i*n+j] = 1.0;
				B[i*n+j] = 1.0;
			}
			
		// 将矩阵A、B的相应数据发送给从进程
		for(i=1; i<num_procs; i++)  
		{
			MPI_Send(B, n*n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
			MPI_Send(A+(i-1)*line*n, line*n, MPI_DOUBLE, i, 1, MPI_COMM_WORLD);
		}
		
		//接收从进程计算结果
		for(i=1; i<num_procs; i++) 
			MPI_Recv(C+(i-1)*line*n, line*n, MPI_DOUBLE, i, 2, MPI_COMM_WORLD, &Status);
			
		//计算剩下的数据
		for(i=(num_procs-1)*line; i<n; i++) 
			for(j=0; j<n; j++) 
			{
				C[i*n+j]=0;
				for(k=0; k<n; k++)
					C[i*n+j]+=A[i*n+k]*B[k*n+j]; 
			}
	} 
	else // 其他进程接收数据，计算结果，发送给主进程
	{  
		MPI_Recv(B, n*n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &Status);
		MPI_Recv(A+(ID-1)*line*n, line*n, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, &Status);
		
		for(i=(ID-1)*line;i<ID*line;i++) 
			for(j=0;j<n;j++) 
			{
				C[i*n+j]=0;
				for(k=0; k<n; k++)
					C[i*n+j]+=A[i*n+k]*B[k*n+j];
			} 
		MPI_Send(C+(num_procs-1)*line*n, line*n, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD);
	}
	MPI_Finalize();
	return 0;
} 
```